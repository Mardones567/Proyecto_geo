{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c729e102",
   "metadata": {},
   "source": [
    "## **Fase 1 (Parte A): Carga de Datos y Definición de Parámetros**\n",
    "\n",
    "**Objetivo:** Preparar nuestro entorno de trabajo. En este paso, definiremos los parámetros clave que controlan nuestros algoritmos y cargaremos el archivo `resultado_geo_viviencias.csv`, convirtiéndolo en un formato geoespacial (`GeoDataFrame`) sobre el cual podamos trabajar.\n",
    "\n",
    "**Parámetros Clave:**\n",
    "* `DBSCAN_EPS`: Es la distancia máxima (en grados) que el algoritmo DBSCAN usará para buscar \"vecinos\". Un valor más grande incluirá más puntos en los clústers. Es el parámetro principal que ajustaremos.\n",
    "* `DBSCAN_MIN_SAMPLES`: El número mínimo de puntos necesarios para ser considerado un \"núcleo\" denso.\n",
    "* `ALPHA_VALUE`: Controla qué tan \"ajustado\" será el polígono final que envuelve los puntos. Un valor más bajo crea una forma más ceñida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6df4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Fase 1 (Versión 2.4 con DBSCAN ajustable)...\n",
      "Parámetros: eps=0.002, min_samples=5, alpha=0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely import wkt\n",
    "import alphashape\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# --- PARÁMETROS DE AJUSTE (¡Puedes experimentar aquí!) ---\n",
    "# -------------------------------------------------------------------\n",
    "# Distancia de búsqueda de DBSCAN en grados. Aumenta este valor para hacer el clúster más grande.\n",
    "# Un buen punto de partida nuevo es 0.0015 (aprox. 166 metros).\n",
    "DBSCAN_EPS = 0.0020\n",
    "\n",
    "# Mínimo de puntos para formar un clúster denso.\n",
    "DBSCAN_MIN_SAMPLES = 5\n",
    "\n",
    "# \"Tensión\" del polígono final. Un valor más bajo lo hace más ajustado.\n",
    "ALPHA_VALUE = 0.1\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(f\"Iniciando Fase 1 (Versión 2.4 con DBSCAN ajustable)...\")\n",
    "print(f\"Parámetros: eps={DBSCAN_EPS}, min_samples={DBSCAN_MIN_SAMPLES}, alpha={ALPHA_VALUE}\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Cargar y Preparar los Datos ---\n",
    "    df = pd.read_csv('resultado_geo_viviencias.csv', low_memory=False)\n",
    "    df.dropna(subset=['geometry', 'Código del Campamento'], inplace=True)\n",
    "    df['geometry'] = df['geometry'].apply(wkt.loads)  # Convertir WKT a geometría\n",
    "    gdf_puntos = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    \n",
    "    # --- 2. Calcular Polígonos con Limpieza de Outliers (DBSCAN) ---\n",
    "    poligonos_lista = []\n",
    "    \n",
    "    for camp_code, group in gdf_puntos.groupby('Código del Campamento'):\n",
    "        if len(group) < DBSCAN_MIN_SAMPLES:\n",
    "            continue  # Ignorar campamentos con menos puntos que el mínimo necesario\n",
    "\n",
    "        # Obtener las coordenadas de los puntos del campamento\n",
    "        coords = np.array(list(zip(group.geometry.x, group.geometry.y)))\n",
    "\n",
    "        # Aplicamos DBSCAN para encontrar los clústeres\n",
    "        db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(coords)\n",
    "        group['cluster'] = db.labels_  # Etiquetar cada punto con su clúster\n",
    "\n",
    "        # Filtrar los puntos que pertenecen a un clúster válido\n",
    "        group_sin_ruido = group[group['cluster'] != -1]  # Los puntos etiquetados como -1 son ruido\n",
    "\n",
    "        if group_sin_ruido.empty:\n",
    "            continue  # Si no hay puntos válidos después de filtrar el ruido, continuamos con el siguiente campamento\n",
    "        \n",
    "        # Encontrar el clúster más grande (con mayor número de puntos)\n",
    "        main_cluster_id = group_sin_ruido['cluster'].value_counts().idxmax()\n",
    "        main_cluster_points = group_sin_ruido[group_sin_ruido['cluster'] == main_cluster_id]\n",
    "\n",
    "        # Generar el polígono si el clúster tiene al menos 3 puntos\n",
    "        if len(main_cluster_points) >= 3:\n",
    "            try:\n",
    "                # Intentamos generar el polígono utilizando Alpha Shape\n",
    "                alpha_shape = alphashape.alphashape(main_cluster_points.geometry, alpha=ALPHA_VALUE)\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': alpha_shape})\n",
    "            except Exception as e:\n",
    "                # Si Alpha Shape falla, usamos el Convex Hull como respaldo\n",
    "                print(f\"Error en Alpha Shape para campamento {camp_code}: {e}\")\n",
    "                convex_shape = main_cluster_points.geometry.unary_union.convex_hull\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': convex_shape})\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en el procesamiento: {e}\")\n",
    "\n",
    "# --- 3. Crear un GeoDataFrame con los polígonos generados ---\n",
    "gdf_poligonos = gpd.GeoDataFrame(poligonos_lista, crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed182e6",
   "metadata": {},
   "source": [
    "## **Fase 1 (Parte B): Clustering y Generación de Polígonos**\n",
    "\n",
    "**Objetivo:** Procesar los puntos de cada campamento para generar un único polígono que represente su huella real, ignorando los puntos atípicos (outliers) que están muy alejados.\n",
    "\n",
    "**Proceso:**\n",
    "Este bloque de código es el núcleo de la Fase 1. Hará lo siguiente para cada `Código del Campamento` único:\n",
    "1.  **Agrupar:** Tomará todos los puntos que pertenecen a ese campamento.\n",
    "2.  **Identificar el Núcleo (DBSCAN):** Aplicará el algoritmo DBSCAN para encontrar el grupo de puntos más denso y grande. Los puntos que estén muy lejos de este núcleo serán clasificados como \"ruido\" y no se usarán para crear el polígono. Esto soluciona el problema de los outliers que vimos en las imágenes.\n",
    "3.  **Crear el Polígono (Alpha Shape):** Finalmente, tomará solo los puntos del \"núcleo\" principal y usará `alphashape` para dibujar un polígono ajustado a su forma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de08553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Fase 1 (Parte B): Clustering y generación de polígonos...\n",
      "\n",
      "¡ÉXITO!\n",
      "Se generaron 18 polígonos de campamentos (post-limpieza de outliers).\n"
     ]
    }
   ],
   "source": [
    "# --- Clustering para limpiar outliers y generación de polígonos ---\n",
    "\n",
    "print(\"Iniciando Fase 1 (Parte B): Clustering y generación de polígonos...\")\n",
    "\n",
    "try:\n",
    "    poligonos_lista = []\n",
    "    # Iteramos sobre cada grupo de campamento único\n",
    "    for camp_code, group in gdf_puntos.groupby('Código del Campamento'):\n",
    "        # Solo procesamos si el campamento tiene suficientes puntos\n",
    "        if len(group) < DBSCAN_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        # Extraemos las coordenadas (x, y) para que DBSCAN trabaje con ellas\n",
    "        coords = np.array(list(zip(group.geometry.x, group.geometry.y)))\n",
    "        \n",
    "        # Aplicamos DBSCAN con los parámetros que definimos en la parte A\n",
    "        db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(coords)\n",
    "        \n",
    "        # Añadimos una columna 'cluster' al grupo para saber a qué cluster pertenece cada punto\n",
    "        group['cluster'] = db.labels_\n",
    "        \n",
    "        # Filtramos los puntos clasificados como ruido (outliers), que tienen el cluster -1\n",
    "        group_sin_ruido = group[group['cluster'] != -1]\n",
    "        \n",
    "        # Si después de quitar el ruido no quedan puntos, pasamos al siguiente campamento\n",
    "        if group_sin_ruido.empty:\n",
    "            continue\n",
    "            \n",
    "        # En caso de que DBSCAN encuentre varios clústers, nos quedamos con el más grande\n",
    "        main_cluster_id = group_sin_ruido['cluster'].value_counts().idxmax()\n",
    "        main_cluster_points = group_sin_ruido[group_sin_ruido['cluster'] == main_cluster_id]\n",
    "\n",
    "        # Necesitamos al menos 3 puntos para formar un polígono\n",
    "        if len(main_cluster_points) >= 3:\n",
    "            try:\n",
    "                # Generamos el polígono SÓLO con los puntos del clúster principal\n",
    "                alpha_shape = alphashape.alphashape(main_cluster_points.geometry, alpha=ALPHA_VALUE)\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': alpha_shape})\n",
    "            except Exception:\n",
    "                # Si alphashape falla por alguna razón (ej. todos los puntos en línea recta),\n",
    "                # usamos el Convex Hull como un método de respaldo seguro.\n",
    "                convex_shape = main_cluster_points.geometry.union_all().convex_hull\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': convex_shape})\n",
    "\n",
    "    # Creamos el GeoDataFrame final con los polígonos generados\n",
    "    gdf_poligonos = gpd.GeoDataFrame(poligonos_lista, crs=\"EPSG:4326\")\n",
    "    \n",
    "    print(\"\\n¡ÉXITO!\")\n",
    "    print(f\"Se generaron {len(gdf_poligonos)} polígonos de campamentos (post-limpieza de outliers).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f346e7b",
   "metadata": {},
   "source": [
    "## **Fase 1 (Parte C): Guardado y Visualización de Resultados**\n",
    "\n",
    "**Objetivo:** Guardar las geometrías que creamos para usarlas en el futuro y generar un mapa interactivo para validar visualmente nuestro trabajo.\n",
    "\n",
    "**Proceso:**\n",
    "1.  **Guardar los Polígonos:** Guardaremos nuestro `gdf_poligonos` en un archivo estándar geoespacial llamado `poligonos_campamentos_ajustados.geojson`. Este archivo es un producto muy valioso, ya que contiene las geometrías limpias que usaremos como base para las Fases 2 y 3.\n",
    "2.  **Crear un Mapa Interactivo:** Generaremos un mapa HTML con la librería `Folium`. Este mapa nos permitirá inspeccionar nuestro trabajo. Dibujaremos los polígonos nuevos (en naranjo) sobre los puntos originales (en azul) para verificar si el ajuste es el correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43344de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Fase 1 (Versión 2.4 con DBSCAN ajustable)...\n",
      "Parámetros: eps=0.002, min_samples=5, alpha=0.1\n",
      "Se generaron 18 polígonos de campamentos (post-ajuste).\n",
      "Los polígonos se han guardado en: 'poligonos_campamentos_ajustados.geojson'\n",
      "Creando mapa interactivo...\n",
      "\n",
      "¡ÉXITO!\n",
      "Busca y abre el archivo 'mapa_poligonos_campamentos_ajustados.html' para ver el nuevo resultado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely import wkt\n",
    "import alphashape\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# --- PARÁMETROS DE AJUSTE (¡Puedes experimentar aquí!) ---\n",
    "# -------------------------------------------------------------------\n",
    "# Distancia de búsqueda de DBSCAN en grados. Aumenta este valor para hacer el clúster más grande.\n",
    "# Un buen punto de partida nuevo es 0.0015 (aprox. 166 metros).\n",
    "DBSCAN_EPS = 0.0020\n",
    "\n",
    "# Mínimo de puntos para formar un clúster denso.\n",
    "DBSCAN_MIN_SAMPLES = 5\n",
    "\n",
    "# \"Tensión\" del polígono final. Un valor más bajo lo hace más ajustado.\n",
    "ALPHA_VALUE = 0.1\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(f\"Iniciando Fase 1 (Versión 2.4 con DBSCAN ajustable)...\")\n",
    "print(f\"Parámetros: eps={DBSCAN_EPS}, min_samples={DBSCAN_MIN_SAMPLES}, alpha={ALPHA_VALUE}\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Cargar y Preparar los Datos ---\n",
    "    df = pd.read_csv('resultado_geo_viviencias.csv', low_memory=False)\n",
    "    df.dropna(subset=['geometry', 'Código del Campamento'], inplace=True)\n",
    "    df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "    gdf_puntos = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    \n",
    "    # --- 2. Calcular Polígonos con Limpieza de Outliers (DBSCAN) ---\n",
    "    poligonos_lista = []\n",
    "    for camp_code, group in gdf_puntos.groupby('Código del Campamento'):\n",
    "        if len(group) < DBSCAN_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        coords = np.array(list(zip(group.geometry.x, group.geometry.y)))\n",
    "        \n",
    "        # Aplicamos DBSCAN con los parámetros definidos arriba\n",
    "        db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(coords)\n",
    "        group['cluster'] = db.labels_\n",
    "        group_sin_ruido = group[group['cluster'] != -1]\n",
    "        \n",
    "        if group_sin_ruido.empty:\n",
    "            continue\n",
    "            \n",
    "        main_cluster_id = group_sin_ruido['cluster'].value_counts().idxmax()\n",
    "        main_cluster_points = group_sin_ruido[group_sin_ruido['cluster'] == main_cluster_id]\n",
    "\n",
    "        if len(main_cluster_points) >= 3:\n",
    "            try:\n",
    "                # Generamos el polígono SÓLO con los puntos del clúster principal\n",
    "                alpha_shape = alphashape.alphashape(main_cluster_points.geometry, alpha=ALPHA_VALUE)\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': alpha_shape})\n",
    "            except Exception:\n",
    "                # Si alphashape falla, usamos el Convex Hull del clúster principal como respaldo\n",
    "                convex_shape = main_cluster_points.geometry.unary_union.convex_hull\n",
    "                poligonos_lista.append({'Código del Campamento': camp_code, 'geometry': convex_shape})\n",
    "\n",
    "    gdf_poligonos = gpd.GeoDataFrame(poligonos_lista, crs=\"EPSG:4326\")\n",
    "    \n",
    "    print(f\"Se generaron {len(gdf_poligonos)} polígonos de campamentos (post-ajuste).\")\n",
    "\n",
    "    # --- 3. Guardar y Visualizar ---\n",
    "    output_geojson_path = 'poligonos_campamentos_ajustados.geojson'\n",
    "    gdf_poligonos.to_file(output_geojson_path, driver='GeoJSON')\n",
    "    print(f\"Los polígonos se han guardado en: '{output_geojson_path}'\")\n",
    "\n",
    "    print(\"Creando mapa interactivo...\")\n",
    "    map_center = [gdf_puntos.geometry.y.mean(), gdf_puntos.geometry.x.mean()]\n",
    "    mapa = folium.Map(location=map_center, zoom_start=11)\n",
    "\n",
    "    # Polígonos en Naranjo para ver la diferencia\n",
    "    folium.GeoJson(\n",
    "        gdf_poligonos,\n",
    "        style_function=lambda x: {'color': 'orange', 'fillColor': 'orange', 'fillOpacity': 0.4, 'weight': 2.5},\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['Código del Campamento'], aliases=['Campamento (Núcleo Ajustado):'])\n",
    "    ).add_to(mapa)\n",
    "\n",
    "    # Puntos originales\n",
    "    for idx, row in gdf_puntos.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x], radius=1, color='blue', fill=True\n",
    "        ).add_to(mapa)\n",
    "    \n",
    "    output_map_path = 'mapa_poligonos_campamentos_ajustados.html'\n",
    "    mapa.save(output_map_path)\n",
    "\n",
    "    print(f\"\\n¡ÉXITO!\")\n",
    "    print(f\"Busca y abre el archivo '{output_map_path}' para ver el nuevo resultado.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884a63f",
   "metadata": {},
   "source": [
    "## **Fase 2: Análisis Temporal con Imágenes Satelitales**\n",
    "\n",
    "**Objetivo:** Responder a la pregunta: **\"¿Han crecido o se han densificado estos campamentos en los últimos años?\"** Para esto, no nos basta con los datos que tenemos; necesitamos \"viajar en el tiempo\" usando imágenes satelitales.\n",
    "\n",
    "**Metodología:**\n",
    "1.  **Herramienta:** Usaremos **Google Earth Engine (GEE)**, una plataforma que nos da acceso a un catálogo inmenso de imágenes satelitales. La primera vez que lo uses, te pedirá que te autentiques con tu cuenta de Google en el navegador.\n",
    "2.  **Índice:** Calcularemos el **NDBI (Índice de Zonas Construidas)**. Este índice resalta las áreas con edificios y concreto. Si el valor promedio de NDBI dentro de uno de nuestros polígonos aumentó entre el pasado (ej. 2019) y el presente (ej. 2024), es una fuerte evidencia de que el asentamiento creció o se densificó.\n",
    "\n",
    "El siguiente código se conectará a GEE, cargará los polígonos que acabamos de crear y calculará el cambio en la densidad de construcción para cada uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfab8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Earth Engine inicializado correctamente.\n",
      "\n",
      "Iniciando análisis temporal (esto puede tardar varios minutos)...\n",
      "Se cargaron 18 polígonos de campamentos para analizar.\n",
      "Procesando campamento: 1310602...\n",
      "  -> NDBI 2019: 0.1161, NDBI 2024: 0.0849, Cambio: -0.0312\n",
      "Procesando campamento: 1311013...\n",
      "  -> NDBI 2019: 0.0585, NDBI 2024: 0.0474, Cambio: -0.0111\n",
      "Procesando campamento: 1311905...\n",
      "  -> NDBI 2019: 0.0678, NDBI 2024: 0.0790, Cambio: +0.0112\n",
      "Procesando campamento: 1311915...\n",
      "  -> NDBI 2019: -0.0397, NDBI 2024: 0.0486, Cambio: +0.0883\n",
      "Procesando campamento: 1311916...\n",
      "  -> NDBI 2019: -0.0502, NDBI 2024: 0.0629, Cambio: +0.1132\n",
      "Procesando campamento: 1312407...\n",
      "  -> NDBI 2019: 0.0673, NDBI 2024: 0.0993, Cambio: +0.0320\n",
      "Procesando campamento: 1312504...\n",
      "  -> NDBI 2019: 0.0944, NDBI 2024: -0.0065, Cambio: -0.1009\n",
      "Procesando campamento: 1312602...\n",
      "  -> NDBI 2019: 0.0038, NDBI 2024: 0.0547, Cambio: +0.0509\n",
      "Procesando campamento: 1312605...\n",
      "  -> NDBI 2019: 0.0802, NDBI 2024: 0.0767, Cambio: -0.0034\n",
      "Procesando campamento: 1312606...\n",
      "  -> NDBI 2019: 0.0257, NDBI 2024: 0.0171, Cambio: -0.0086\n",
      "Procesando campamento: 1320103...\n",
      "  -> NDBI 2019: 0.1127, NDBI 2024: 0.1373, Cambio: +0.0246\n",
      "Procesando campamento: 1320105...\n",
      "  -> NDBI 2019: 0.0858, NDBI 2024: 0.0865, Cambio: +0.0007\n",
      "Procesando campamento: 1320111...\n",
      "  -> NDBI 2019: 0.0743, NDBI 2024: 0.0989, Cambio: +0.0246\n",
      "Procesando campamento: 1320113...\n",
      "  -> NDBI 2019: 0.0933, NDBI 2024: 0.0772, Cambio: -0.0161\n",
      "Procesando campamento: 1330221...\n",
      "  -> NDBI 2019: 0.1444, NDBI 2024: 0.0654, Cambio: -0.0790\n",
      "Procesando campamento: 1360104...\n",
      "  -> NDBI 2019: -0.0996, NDBI 2024: -0.0253, Cambio: +0.0743\n",
      "Procesando campamento: 1360109...\n",
      "  -> NDBI 2019: -0.2525, NDBI 2024: -0.2643, Cambio: -0.0118\n",
      "Procesando campamento: 1360206...\n",
      "  -> NDBI 2019: -0.2019, NDBI 2024: -0.1725, Cambio: +0.0294\n",
      "\n",
      "--- Resultados del Análisis de Crecimiento ---\n",
      "    Código del Campamento  NDBI_2019  NDBI_2024  Cambio_NDBI\n",
      "0                 1310602   0.116093   0.084879    -0.031215\n",
      "1                 1311013   0.058465   0.047352    -0.011112\n",
      "2                 1311905   0.067808   0.078996     0.011188\n",
      "3                 1311915  -0.039686   0.048622     0.088308\n",
      "4                 1311916  -0.050249   0.062901     0.113150\n",
      "5                 1312407   0.067297   0.099305     0.032008\n",
      "6                 1312504   0.094392  -0.006503    -0.100894\n",
      "7                 1312602   0.003822   0.054674     0.050852\n",
      "8                 1312605   0.080167   0.076747    -0.003419\n",
      "9                 1312606   0.025694   0.017121    -0.008573\n",
      "10                1320103   0.112734   0.137322     0.024589\n",
      "11                1320105   0.085776   0.086519     0.000743\n",
      "12                1320111   0.074312   0.098887     0.024575\n",
      "13                1320113   0.093350   0.077221    -0.016129\n",
      "14                1330221   0.144404   0.065395    -0.079009\n",
      "15                1360104  -0.099594  -0.025316     0.074278\n",
      "16                1360109  -0.252493  -0.264318    -0.011825\n",
      "17                1360206  -0.201928  -0.172536     0.029392\n",
      "\n",
      "¡ÉXITO! Fase 2 completada.\n",
      "Resultados guardados en 'resultados_crecimiento_campamentos_ndbi.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Importar la librería de Earth Engine ---\n",
    "import ee\n",
    "\n",
    "# --- Autenticar e Inicializar Earth Engine ---\n",
    "try:\n",
    "    # Intenta inicializar. Si ya hay credenciales guardadas, funcionará directamente.\n",
    "    ee.Initialize()\n",
    "    print(\"Google Earth Engine inicializado correctamente.\")\n",
    "except Exception as e:\n",
    "    # Si es la primera vez o las credenciales expiraron, pide autenticación.\n",
    "    # Se abrirá una ventana en tu navegador para que inicies sesión y copies un código.\n",
    "    print(\"Autenticando con Google Earth Engine...\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Google Earth Engine inicializado correctamente.\")\n",
    "\n",
    "\n",
    "# --- Función para Calcular NDBI Promedio ---\n",
    "def calcular_ndbi_promedio(poligono_ee, anio):\n",
    "    \"\"\"Calcula el NDBI promedio para un polígono y año específicos.\"\"\"\n",
    "    fecha_inicio = f'{anio}-01-01'\n",
    "    fecha_fin = f'{anio}-12-31'\n",
    "    \n",
    "    # Usar imágenes de Sentinel-2, un satélite de alta resolución (10m)\n",
    "    coleccion = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "                  .filterDate(fecha_inicio, fecha_fin) \\\n",
    "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "                  .filterBounds(poligono_ee)\n",
    "\n",
    "    # Calcular la mediana de las imágenes para obtener una imagen compuesta y sin nubes\n",
    "    imagen = coleccion.median()\n",
    "    \n",
    "    # Si no hay imágenes válidas en el período, la mediana estará vacía. Lo comprobamos.\n",
    "    if imagen.bandNames().size().getInfo() == 0:\n",
    "        return None\n",
    "\n",
    "    # Calcular NDBI = (SWIR1 - NIR) / (SWIR1 + NIR)\n",
    "    # Para Sentinel-2, las bandas son: B11 (SWIR1) y B8 (NIR)\n",
    "    ndbi = imagen.normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
    "    \n",
    "    # Calcular el valor promedio de NDBI dentro del polígono\n",
    "    estadisticas = ndbi.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=poligono_ee,\n",
    "        scale=10,  # Resolución en metros de las bandas de Sentinel-2\n",
    "        maxPixels=1e9\n",
    "    )\n",
    "    \n",
    "    # getInfo() es la llamada que trae los datos desde el servidor de GEE a nuestro script\n",
    "    return estadisticas.get('NDBI').getInfo()\n",
    "\n",
    "# --- Cargar nuestros polígonos y ejecutar el análisis ---\n",
    "print(\"\\nIniciando análisis temporal (esto puede tardar varios minutos)...\")\n",
    "resultados = []\n",
    "\n",
    "try:\n",
    "    gdf_poligonos = gpd.read_file('poligonos_campamentos_ajustados.geojson')\n",
    "    print(f\"Se cargaron {len(gdf_poligonos)} polígonos de campamentos para analizar.\")\n",
    "\n",
    "    for index, row in gdf_poligonos.iterrows():\n",
    "        codigo_campamento = row['Código del Campamento']\n",
    "        print(f\"Procesando campamento: {codigo_campamento}...\")\n",
    "        \n",
    "        # Convertir el polígono de geopandas a un objeto de Earth Engine\n",
    "        geom_json = row.geometry.__geo_interface__\n",
    "        poligono_ee = ee.Geometry(geom_json)\n",
    "        \n",
    "        # Calcular NDBI para dos períodos de tiempo: 2019 y 2024\n",
    "        ndbi_pasado = calcular_ndbi_promedio(poligono_ee, 2019)\n",
    "        ndbi_presente = calcular_ndbi_promedio(poligono_ee, 2024)\n",
    "\n",
    "        cambio = None\n",
    "        # Solo calculamos el cambio si ambos valores son válidos\n",
    "        if ndbi_pasado is not None and ndbi_presente is not None:\n",
    "            cambio = ndbi_presente - ndbi_pasado\n",
    "            print(f\"  -> NDBI 2019: {ndbi_pasado:.4f}, NDBI 2024: {ndbi_presente:.4f}, Cambio: {cambio:+.4f}\")\n",
    "        else:\n",
    "            print(f\"  -> No se encontraron imágenes suficientes para uno o ambos años.\")\n",
    "\n",
    "        resultados.append({\n",
    "            'Código del Campamento': codigo_campamento,\n",
    "            'NDBI_2019': ndbi_pasado,\n",
    "            'NDBI_2024': ndbi_presente,\n",
    "            'Cambio_NDBI': cambio\n",
    "        })\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"No se pudo encontrar el archivo 'poligonos_campamentos_ajustados.geojson'. Asegúrate de haber completado la Fase 1.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")\n",
    "\n",
    "# --- Mostrar y Guardar Resultados Finales ---\n",
    "if resultados:\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    print(\"\\n--- Resultados del Análisis de Crecimiento ---\")\n",
    "    print(df_resultados)\n",
    "\n",
    "    df_resultados.to_csv('resultados_crecimiento_campamentos_ndbi.csv', index=False)\n",
    "    print(\"\\n¡ÉXITO! Fase 2 completada.\")\n",
    "    print(\"Resultados guardados en 'resultados_crecimiento_campamentos_ndbi.csv'\")\n",
    "else:\n",
    "    print(\"\\nNo se pudo generar ningún resultado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76162712",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## **Fase 3: Creación del Set de Datos Final para el Modelo**\n",
    "\n",
    "**Objetivo:** Construir nuestra tabla \"maestra\". Cada fila de esta tabla será un **distrito censal**, y las columnas serán todas las características que hemos preparado y calculado sobre él.\n",
    "\n",
    "**Proceso:**\n",
    "1.  **Cargar Todos los Insumos:** Cargaremos los shapefiles de Distritos y Ejes Viales, junto con los dos archivos que generamos en las fases anteriores (`poligonos_campamentos_ajustados.geojson` y `resultados_crecimiento_campamentos_ndbi.csv`).\n",
    "2.  **Proyectar a un CRS Métrico:** Convertiremos todas las capas a un sistema de coordenadas que use metros (UTM, EPSG:32719) para que los cálculos de distancia y área sean precisos.\n",
    "3.  **Unir y Calcular Características:** Realizaremos una serie de uniones (`joins`) para asignar a cada distrito:\n",
    "    * El número de campamentos que contiene.\n",
    "    * El cambio promedio de NDBI de esos campamentos.\n",
    "    * La distancia a la red vial más cercana.\n",
    "4.  **Guardar el Dataset Final:** El resultado será un único archivo (`dataset_listo_para_entrenar.geojson`) con toda esta inteligencia consolidada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b758423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Fase 3: Creación del set de datos para el modelo...\n",
      "Cargando y proyectando archivos base...\n",
      "Calculando características de campamentos y NDBI por distrito...\n",
      "Calculando distancia a la red vial para cada distrito...\n",
      "Preparando y guardando el dataset final...\n",
      "\n",
      "¡ÉXITO! Fase 3 completada.\n",
      "Set de datos listo para el modelo guardado en: 'dataset_final_para_modelo.geojson'\n",
      "\n",
      "Vista previa de las primeras filas del dataset final:\n",
      "   COD_DISTRI  N_COMUNA  SHAPE_Area  SHAPE_Leng  n_campamentos  \\\n",
      "0           9  SANTIAGO    0.000085    0.041509              0   \n",
      "1           7  SANTIAGO    0.000053    0.032889              0   \n",
      "2           8  SANTIAGO    0.000065    0.035540              0   \n",
      "3           3  SANTIAGO    0.000054    0.039767              0   \n",
      "4           2  SANTIAGO    0.000117    0.044109              0   \n",
      "\n",
      "   cambio_ndbi_promedio  distancia_vial_m  \\\n",
      "0                   0.0         26.731173   \n",
      "1                   0.0          4.283775   \n",
      "2                   0.0         25.124379   \n",
      "3                   0.0         15.179970   \n",
      "4                   0.0         32.363057   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((345539.227 6298996.161, 345454.4 629...  \n",
      "1  POLYGON ((344160.874 6299496.995, 344160.892 6...  \n",
      "2  POLYGON ((344832.9 6299553.6, 344839.95 629947...  \n",
      "3  POLYGON ((345645.563 6298041.012, 345644.514 6...  \n",
      "4  POLYGON ((346005.656 6299723.64, 346007.145 62...  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Iniciando Fase 3: Creación del set de datos para el modelo...\")\n",
    "\n",
    "# Define el CRS proyectado para Santiago (UTM Zona 19S) para cálculos en metros\n",
    "CRS_PROYECTADO = \"EPSG:32719\"\n",
    "\n",
    "try:\n",
    "    # --- 1. Cargar todas las fuentes de datos y proyectarlas ---\n",
    "    print(\"Cargando y proyectando archivos base...\")\n",
    "    # El shapefile con TODOS los distritos de tu área de estudio\n",
    "    gdf_distritos = gpd.read_file('SHP_APC2023_R13/Distrital.shp').to_crs(CRS_PROYECTADO)\n",
    "    \n",
    "    # Las geometrías de los campamentos que creamos en Fase 1\n",
    "    gdf_camp_poligonos = gpd.read_file('poligonos_campamentos_ajustados.geojson').to_crs(CRS_PROYECTADO)\n",
    "    \n",
    "    # La red de calles\n",
    "    gdf_vial = gpd.read_file('SHP_APC2023_R13/Eje_Vial.shp').to_crs(CRS_PROYECTADO)\n",
    "    \n",
    "    # Los resultados del análisis de crecimiento (NDBI) de la Fase 2\n",
    "    df_crecimiento = pd.read_csv('resultados_crecimiento_campamentos_ndbi.csv')\n",
    "\n",
    "    # --- 2. Enriquecer los distritos con datos de campamentos y NDBI ---\n",
    "    print(\"Calculando características de campamentos y NDBI por distrito...\")\n",
    "    \n",
    "    # Unir los datos de crecimiento (NDBI) a los polígonos de campamento\n",
    "    gdf_camp_poligonos = gdf_camp_poligonos.merge(df_crecimiento, on='Código del Campamento', how='left')\n",
    "    \n",
    "    # Unir espacialmente los campamentos (con NDBI) a los distritos\n",
    "    join_dist_camp = gpd.sjoin(gdf_distritos, gdf_camp_poligonos, how='left', predicate='intersects')\n",
    "    \n",
    "    # Agrupar por distrito para calcular las métricas\n",
    "    # Contamos campamentos únicos y calculamos el NDBI promedio\n",
    "    stats_por_distrito = join_dist_camp.groupby(join_dist_camp.index).agg(\n",
    "        n_campamentos=('Código del Campamento', 'nunique'),\n",
    "        cambio_ndbi_promedio=('Cambio_NDBI', 'mean')\n",
    "    )\n",
    "    \n",
    "    # Unimos las estadísticas al GeoDataFrame de distritos original\n",
    "    gdf_distritos = gdf_distritos.merge(stats_por_distrito, left_index=True, right_index=True, how='left').fillna(0)\n",
    "\n",
    "    # --- 3. Calcular distancia a la vía más cercana ---\n",
    "    print(\"Calculando distancia a la red vial para cada distrito...\")\n",
    "    \n",
    "    # Unificar todas las calles en una sola geometría para un cálculo más rápido\n",
    "    red_vial_unificada = gdf_vial.geometry.union_all()\n",
    "    \n",
    "    # Calcular la distancia desde el centroide de cada distrito a la red vial unificada\n",
    "    gdf_distritos['distancia_vial_m'] = gdf_distritos.geometry.centroid.apply(\n",
    "        lambda punto: punto.distance(red_vial_unificada)\n",
    "    )\n",
    "\n",
    "    # --- 4. Preparar y guardar el resultado final ---\n",
    "    print(\"Preparando y guardando el dataset final...\")\n",
    "    \n",
    "    # Seleccionamos las columnas que usaremos en el modelo\n",
    "    gdf_distritos['COD_DISTRI'] = gdf_distritos['COD_DISTRI'].astype(int)\n",
    "    \n",
    "    columnas_finales = [\n",
    "        'COD_DISTRI', 'N_COMUNA', 'SHAPE_Area', 'SHAPE_Leng', \n",
    "        'n_campamentos', 'cambio_ndbi_promedio', 'distancia_vial_m', \n",
    "        'geometry'\n",
    "    ]\n",
    "    \n",
    "    dataset_final = gdf_distritos[columnas_finales]\n",
    "    \n",
    "    # Guardar el GeoDataFrame final\n",
    "    output_path = 'dataset_final_para_modelo.geojson'\n",
    "    dataset_final.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "    print(f\"\\n¡ÉXITO! Fase 3 completada.\")\n",
    "    print(f\"Set de datos listo para el modelo guardado en: '{output_path}'\")\n",
    "    print(\"\\nVista previa de las primeras filas del dataset final:\")\n",
    "    print(dataset_final.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n--- ERROR DE ARCHIVO ---\")\n",
    "    print(f\"No se pudo encontrar un archivo necesario: {e}\")\n",
    "    print(\"Asegúrate de que todos los archivos SHP y los resultados de las fases 1 y 2 estén disponibles.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec81606",
   "metadata": {},
   "source": [
    "---\n",
    "### **Preparación de Datos Socioeconómicos (CASEN 2022)**\n",
    "\n",
    "**Objetivo:** Tomar los dos archivos CSV que contienen las estimaciones de pobreza de la encuesta CASEN 2022, procesarlos para que sean utilizables, y unirlos en una única tabla (`df_casen_final`) que contenga las tasas de pobreza por comuna.\n",
    "\n",
    "**Proceso Detallado:**\n",
    "\n",
    "1.  **Carga de Datos:** Se cargan los dos archivos CSV (`Estimaciones_Tasa_Pobreza_Ingresos...` y `Estimaciones_Indice_Pobreza_Multidimensional...`) que ya tienes en tu entorno, especificando que se deben omitir las primeras filas de encabezado (`header=2`).\n",
    "\n",
    "2.  **Limpieza de Pobreza por Ingresos:**\n",
    "    * Se seleccionan únicamente las dos columnas que nos interesan: `'Nombre comuna'` y la columna con el porcentaje.\n",
    "    * Se les cambia el nombre a unos más cortos y programáticos: `NOMBRE_COMUNA` y `tasa_pobreza_ingresos`.\n",
    "    * Se transforma la columna de la tasa de pobreza (que es un texto, ej: \"8,5%\") a un número decimal normalizado (ej: 0.085). Para esto, se reemplaza la coma por un punto, se elimina el símbolo `%` y se divide por 100.\n",
    "\n",
    "3.  **Limpieza de Pobreza Multidimensional:**\n",
    "    * Se repite exactamente el mismo proceso de selección, renombre y conversión para el segundo archivo, creando la columna `tasa_pobreza_multi`.\n",
    "\n",
    "4.  **Unión de las Tablas:**\n",
    "    * Una vez que ambas tablas están limpias, se unen en un único DataFrame (`df_casen_final`) usando el nombre de la comuna como llave.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6339f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la preparación de datos socioeconómicos (CASEN 2022)...\n",
      "Limpiando datos de pobreza por ingresos...\n",
      "Limpiando datos de pobreza multidimensional...\n",
      "Tablas económicas unificadas exitosamente.\n",
      "\n",
      "¡ÉXITO!\n",
      "Vista previa del DataFrame con los datos de CASEN limpios y unidos:\n",
      "     NOMBRE_COMUNA  tasa_pobreza_ingresos  tasa_pobreza_multi\n",
      "0        Algarrobo                  0.061               0.196\n",
      "1            Alhué                  0.096               0.291\n",
      "2      Alto Biobío                  0.221               0.389\n",
      "3  Alto Del Carmen                  0.151               0.279\n",
      "4    Alto Hospicio                  0.153               0.326\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Iniciando la preparación de datos socioeconómicos (CASEN 2022)...\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Cargar los archivos CSV originales ---\n",
    "    # Asumimos que ya tienes estos DataFrames cargados.\n",
    "    # Si no, descomenta estas líneas y asegúrate de que los nombres de archivo sean correctos.\n",
    "    df_economico = pd.read_csv('Estimaciones_Tasa_Pobreza_Ingresos_Comunas_2022(Estimaciones).csv', encoding='latin1', header=2)\n",
    "    df_economico_multidimensional = pd.read_csv('Estimaciones_Indice_Pobreza_Multidimensional_Comunas_2022 1(Estimaciones).csv', header=2, encoding='latin1')\n",
    "\n",
    "    # --- 2. Limpiar datos de Pobreza por Ingresos ---\n",
    "    print(\"Limpiando datos de pobreza por ingresos...\")\n",
    "    pobreza_ingresos = df_economico[['Nombre comuna', 'Porcentaje de personas en situación de pobreza por ingresos 2022']].copy()\n",
    "    pobreza_ingresos.rename(columns={\n",
    "        'Nombre comuna': 'NOMBRE_COMUNA',\n",
    "        'Porcentaje de personas en situación de pobreza por ingresos 2022': 'tasa_pobreza_ingresos'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Convertir la columna de pobreza a un número (float) entre 0 y 1\n",
    "    pobreza_ingresos['tasa_pobreza_ingresos'] = pobreza_ingresos['tasa_pobreza_ingresos'] \\\n",
    "        .astype(str) \\\n",
    "        .str.replace(',', '.', regex=False) \\\n",
    "        .str.replace('%', '', regex=False) \\\n",
    "        .astype(float) / 100.0\n",
    "    \n",
    "    # --- 3. Limpiar datos de Pobreza Multidimensional ---\n",
    "    print(\"Limpiando datos de pobreza multidimensional...\")\n",
    "    # Usamos el nombre de columna correcto que identificamos previamente\n",
    "    columna_correcta_multi = 'Porcentaje de personas en situación de pobreza multidimensional 2022'\n",
    "    \n",
    "    pobreza_multi = df_economico_multidimensional[['Nombre comuna', columna_correcta_multi]].copy()\n",
    "    pobreza_multi.rename(columns={\n",
    "        'Nombre comuna': 'NOMBRE_COMUNA',\n",
    "        columna_correcta_multi: 'tasa_pobreza_multi'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Convertir la columna de pobreza a un número (float) entre 0 y 1\n",
    "    pobreza_multi['tasa_pobreza_multi'] = pobreza_multi['tasa_pobreza_multi'] \\\n",
    "        .astype(str) \\\n",
    "        .str.replace(',', '.', regex=False) \\\n",
    "        .str.replace('%', '', regex=False) \\\n",
    "        .astype(float) / 100.0\n",
    "\n",
    "    # --- 4. Unir los dos dataframes económicos en uno solo ---\n",
    "    df_casen_final = pd.merge(pobreza_ingresos, pobreza_multi, on='NOMBRE_COMUNA', how='outer')\n",
    "    print(\"Tablas económicas unificadas exitosamente.\")\n",
    "\n",
    "    # --- 5. Mostrar el resultado ---\n",
    "    print(\"\\n¡ÉXITO!\")\n",
    "    print(\"Vista previa del DataFrame con los datos de CASEN limpios y unidos:\")\n",
    "    print(df_casen_final.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n--- ERROR DE ARCHIVO ---\")\n",
    "    print(f\"No se pudo encontrar uno de los archivos CSV de CASEN: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERROR DE COLUMNA ---\")\n",
    "    print(f\"No se encontró el nombre de columna esperado: {e}. Revisa los nombres en tus archivos CSV.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea1abc",
   "metadata": {},
   "source": [
    "¡Exacto! Ese es el último paso de la **Fase 3**. Ahora vamos a tomar nuestro GeoDataFrame de distritos y lo vamos a enriquecer con la tabla de datos socioeconómicos que acabamos de limpiar.\n",
    "\n",
    "---\n",
    "## **Fase 3 (Parte Final): Unión de Datos Socioeconómicos**\n",
    "\n",
    "**Objetivo:** Añadir las columnas con las tasas de pobreza del DataFrame `df_casen_final` a nuestro GeoDataFrame principal `gdf_distritos`.\n",
    "\n",
    "**Proceso:**\n",
    "Lo haremos usando el nombre de la comuna como \"llave\" para conectar ambas tablas. Un paso crucial en cualquier unión de datos es estandarizar las llaves para asegurar que coincidan perfectamente. En este caso, convertiremos los nombres de las comunas en ambas tablas a mayúsculas y les quitaremos cualquier espacio extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb482649",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la unión final de datos...\n",
      "Uniendo datos CASEN al GeoDataFrame de distritos...\n",
      "\n",
      "¡ÉXITO! Fase 3 completada.\n",
      "Dataset final y enriquecido guardado en: 'dataset_listo_para_entrenar.geojson'\n",
      "\n",
      "Vista previa de las primeras filas del dataset final con todas las características:\n",
      "   COD_DISTRI  N_COMUNA  n_campamentos  distancia_vial_m  \\\n",
      "0           9  SANTIAGO              0         26.731173   \n",
      "1           7  SANTIAGO              0          4.283775   \n",
      "2           8  SANTIAGO              0         25.124379   \n",
      "3           3  SANTIAGO              0         15.179970   \n",
      "4           2  SANTIAGO              0         32.363057   \n",
      "\n",
      "   tasa_pobreza_ingresos  tasa_pobreza_multi  \n",
      "0                  0.039               0.165  \n",
      "1                  0.039               0.165  \n",
      "2                  0.039               0.165  \n",
      "3                  0.039               0.165  \n",
      "4                  0.039               0.165  \n",
      "\n",
      "Número de distritos sin datos CASEN correspondientes: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Iniciando la unión final de datos...\")\n",
    "\n",
    "try:\n",
    "    # --- Estandarizar las llaves para la unión ---\n",
    "    # En gdf_distritos la columna es 'N_COMUNA'\n",
    "    # En df_casen_final la columna es 'NOMBRE_COMUNA'\n",
    "    \n",
    "    # Limpiamos y convertimos a mayúsculas para asegurar una coincidencia perfecta\n",
    "    gdf_distritos['N_COMUNA_JOIN_KEY'] = gdf_distritos['N_COMUNA'].str.strip().str.upper()\n",
    "    df_casen_final['N_COMUNA_JOIN_KEY'] = df_casen_final['NOMBRE_COMUNA'].str.strip().str.upper()\n",
    "\n",
    "    # --- Realizar la unión (merge) ---\n",
    "    print(\"Uniendo datos CASEN al GeoDataFrame de distritos...\")\n",
    "    \n",
    "    # Usamos pd.merge. 'how=left' para mantener todos los distritos de nuestro gdf original.\n",
    "    gdf_enriquecido = pd.merge(\n",
    "        gdf_distritos,\n",
    "        df_casen_final,\n",
    "        on='N_COMUNA_JOIN_KEY', # La llave común\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- Limpiar y guardar ---\n",
    "    # Eliminamos las columnas que ya no necesitamos\n",
    "    gdf_enriquecido = gdf_enriquecido.drop(columns=['NOMBRE_COMUNA', 'N_COMUNA_JOIN_KEY'])\n",
    "    \n",
    "    output_path = 'dataset_listo_para_entrenar.geojson'\n",
    "    gdf_enriquecido.to_file(output_path, driver='GeoJSON')\n",
    "    \n",
    "    print(\"\\n¡ÉXITO! Fase 3 completada.\")\n",
    "    print(f\"Dataset final y enriquecido guardado en: '{output_path}'\")\n",
    "    \n",
    "    print(\"\\nVista previa de las primeras filas del dataset final con todas las características:\")\n",
    "    # Mostramos las columnas más importantes para verificar\n",
    "    columnas_a_mostrar = [\n",
    "        'COD_DISTRI', 'N_COMUNA', 'n_campamentos', \n",
    "        'distancia_vial_m', 'tasa_pobreza_ingresos', 'tasa_pobreza_multi'\n",
    "    ]\n",
    "    print(gdf_enriquecido[columnas_a_mostrar].head())\n",
    "    \n",
    "    # Verifiquemos si alguna comuna no tuvo datos CASEN\n",
    "    null_check = gdf_enriquecido['tasa_pobreza_ingresos'].isnull().sum()\n",
    "    print(f\"\\nNúmero de distritos sin datos CASEN correspondientes: {null_check}\")\n",
    "\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERROR DE COLUMNA ---\")\n",
    "    print(f\"No se encontró el nombre de columna esperado: {e}. Asegúrate de haber ejecutado los pasos anteriores.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcurrió un error inesperado: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
